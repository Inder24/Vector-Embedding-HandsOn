{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8177622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the tiktoken library\n",
    "!pip install tiktoken\n",
    "\n",
    "# Import the library\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2028, 374, 6205, 1495, 662, 58166, 1518, 279, 11460]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "#this is the model used by gpt 4 for tokenization\n",
    "\n",
    "user_input= input(\"Enter Text Here\")\n",
    "\n",
    "tokens=tokenizer.encode(user_input)\n",
    "\n",
    "print(tokens)\n",
    "#This will create the tokens for your entered text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c1e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is sample text . Lets see the tokens\n"
     ]
    }
   ],
   "source": [
    "decode = tokenizer.decode(tokens)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd13f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5319, 2779, 553, 420, 11914, 323, 35883, 279, 2612]\n",
      "[b'TO', b'ken', b'ize', b' this', b' sentence', b' and', b' showcase', b' the', b' output']\n"
     ]
    }
   ],
   "source": [
    "user_input=input(\"\")\n",
    "tokens = tokenizer.encode(user_input)\n",
    "decode_to_bytes = tokenizer.decode_tokens_bytes(tokens)\n",
    "print(tokens)\n",
    "print(decode_to_bytes)\n",
    "\n",
    "#Tokenize this sentence and showcase the requirement . You'll see space is not a token itself but appended with the words.\n",
    "# Also you'll see how thw words have been broken down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d7a09be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;47;1mThis\u001b[0m\u001b[0;42;1m is\u001b[0m\u001b[0;43;1m a\u001b[0m\u001b[0;44;1m sample\u001b[0m\u001b[0;46;1m token\u001b[0m\u001b[0;45;1mization\u001b[0m\u001b[0;47;1m of\u001b[0m\u001b[0;42;1m this\u001b[0m\u001b[0;43;1m sentence\u001b[0m\u001b[0;44;1m as\u001b[0m\u001b[0;46;1m required\u001b[0m\u001b[0;45;1m to\u001b[0m\u001b[0;47;1m understand\u001b[0m\u001b[0;42;1m the\u001b[0m\u001b[0;43;1m ou\u001b[0m\u001b[0;44;1mput\u001b[0m\u001b[0;46;1m.\u001b[0m\u001b[0;45;1m Also\u001b[0m\u001b[0;47;1m we\u001b[0m\u001b[0;42;1m will\u001b[0m\u001b[0;43;1m color\u001b[0m\u001b[0;44;1m this\u001b[0m\u001b[0;46;1m sentence\u001b[0m\n",
      "\n",
      "['This', ' is', ' a', ' sample', ' token', 'ization', ' of', ' this', ' sentence', ' as', ' required', ' to', ' understand', ' the', ' ou', 'put', '.', ' Also', ' we', ' will', ' color', ' this', ' sentence']\n",
      "\n",
      "[2028, 374, 264, 6205, 4037, 2065, 315, 420, 11914, 439, 2631, 311, 3619, 279, 6033, 631, 13, 7429, 584, 690, 1933, 420, 11914]\n",
      "\n",
      "Token Count: 23\n",
      "Characters: 116\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "count = 0\n",
    "token_list = []\n",
    "\n",
    "user_input= input(\"\")\n",
    "clear_output(wait=True)\n",
    "\n",
    "encode = tokenizer.encode(user_input)\n",
    "decode = tokenizer.decode_tokens_bytes(encode)\n",
    "\n",
    "for token in decode:\n",
    "    token_list.append(token.decode())\n",
    "\n",
    "character_count = sum(len(i) for i in token_list)\n",
    "length = len(encode)\n",
    "#This is basically used for coloring each token and repeat the color after 6.\n",
    "for tk in token_list:\n",
    "    if count == 0:\n",
    "        print('\\x1b[0;47;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 1:\n",
    "        print('\\x1b[0;42;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 2:\n",
    "        print('\\x1b[0;43;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 3:\n",
    "        print('\\x1b[0;44;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 4:\n",
    "        print('\\x1b[0;46;1m' + tk + '\\x1b[0m', end='')\n",
    "        count+=1\n",
    "    elif count == 5:\n",
    "        print('\\x1b[0;45;1m' + tk + '\\x1b[0m', end='')\n",
    "        count=0      \n",
    "\n",
    "print(\"\\n\\n\" + str(token_list) +\"\\n\")\n",
    "print(str(encode) + \"\\n\")\n",
    "print(\"Token Count: \" + str(length))\n",
    "print(\"Characters: \" + str(character_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
